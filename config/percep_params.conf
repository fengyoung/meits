# Model name, it's must be 'Perceptron'
Model = Perceptron

#################################
# Learning Parameters

# Type of optimization
# There are 7 types, include SGD, Momentum, NAG(Nesterov Accelerated Gradient), AdaGrad, RMSprop, AdaDelta and Adam. 
# Adam is RECOMMENDED!
Optim = Adam

# Common Learning Parameters
# size of mini-batch, 0 for batch, 1 for pure stochastic. 50~200 is RECOMMENDED.
BatchSize = 200
# max epoches for the whole training, 200 is RECOMMENDED.
MaxEpoches = 200
# max epoches for training early-stop, 0 for disabled, 10 is RECOMMENDED.
EarlyStop = 10
# threshold of loss value, when the value less then epsilon, the training would be stopped.
Epsilon = 0.1

# Learning Parameters of SGD
SGD_Regula = L1
SGD_Lambda = 0.01
SGD_LearningRateInit = 0.001


# Learning Parameters of Momentum
Momentum_Beta = 0.9
Momentum_LearningRateInit = 0.001


# Learning Parameters of NAG
NAG_Beta = 0.9
NAG_LearningRateInit = 0.001


# Learning Parameters of AdaGrad
AdaGrad_Eps = 1e-6
AdaGrad_LearningRateInit = 0.1


# Learning Parameters of RMSprop
RMSprop_Beta = 0.9
RMSprop_Eps = 1e-6
RMSprop_LearningRateInit = 0.001


# Learning Parameters of AdaDelta
AdaDelta_Rho = 0.9
AdaDelta_Beta = 0.9
AdaDelta_Eps = 1e-8


# Learning Parameters of Adam
Adam_Beta1 = 0.9
Adam_Beta2 = 0.999
Adam_Eps = 1e-8
Adam_LearningRateInit = 0.001




